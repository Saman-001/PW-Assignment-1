{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "R-squared (also known as the coefficient of determination) is a statistical measure used in linear regression models to indicate how well the independent variables (predictors) explain the variability in the dependent variable (response). It represents the proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "Range: 0 to 1.\n",
    "-0: The model explains none of the variability.\n",
    "-1: The model explains all the variability.\n",
    "-Example: If R-squared = 0.8 (80%), 80% of the variation is explained by the model.\n",
    "\n",
    "Formula:-\n",
    "\n",
    "R2 = 1 - ( SS res/SS total )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Adjusted R-squared is a modified version of R-squared that accounts for the number of independent variables (predictors) in a regression model. It adjusts the R-squared value by penalizing models with more predictors, making it a better measure when comparing models with different numbers of variables.\n",
    "\n",
    "##### Difference\n",
    "R-squared measures the proportion of variance explained by the model and always increases with more variables. Adjusted R-squared penalizes for adding unnecessary predictors, increasing only if they improve the model.\n",
    "\n",
    "R2(adj) = 1-  ( (1- R2) (n-1) ) / n-p-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Adjusted R-squared is more appropriate when evaluating the goodness of fit of a regression model with multiple predictive features. Unlike R-squared, it adjusts for the number of features in the model, penalizing models with more features to prevent overfitting. Therefore, it provides a more reliable measure of the contribution of each new feature to the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "1. Root Mean Squared Error (RMSE): It's the square root of the mean of the squared differences between predicted and actual values. RMSE penalizes large errors, providing a measure of the dispersion of errors. Lower values indicate better model performance.\n",
    "\n",
    "2. Mean Squared Error (MSE): It's the average of the squared differences between predicted and actual values. MSE is sensitive to large errors, making it a good measure for models where large errors are particularly undesirable.\n",
    "\n",
    "3. Mean Absolute Error (MAE): It's the average of the absolute differences between predicted and actual values. MAE is less sensitive to outliers and provides a better measure of average error magnitude.\n",
    "\n",
    "These metrics are calculated as follows:\n",
    "\n",
    "- RMSE: sqrt(sum((y_pred - y_true)^2) / n)\n",
    "- MSE: sum((y_pred - y_true)^2) / n\n",
    "- MAE: sum(abs(y_pred - y_true)) / n\n",
    "\n",
    "Where y_pred is the predicted value, y_true is the true value, and n is the number of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "Root Mean Square Error (RMSE), Mean Squared Error (MSE), and Mean Absolute Error (MAE) are common metrics for regression analysis.\n",
    "\n",
    "##### Advantages:\n",
    "\n",
    "1. RMSE and MSE are sensitive to large errors, making them useful when large errors are particularly undesirable.\n",
    "2. MAE is easier to interpret as it is in the same units as the target variable.\n",
    "\n",
    "##### Disadvantages:\n",
    "\n",
    "1. RMSE and MSE are sensitive to outliers, which can skew results.\n",
    "2. MSE is not easily interpretable as it's in squared units.\n",
    "3. MAE doesn't consider the difference in the size of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) and Ridge (L2 regularization) are techniques used in linear regression to prevent overfitting.\n",
    "\n",
    "Lasso regularization adds an absolute value of the magnitude of coefficient as penalty term to the loss function. It can lead to zero coefficients, effectively eliminating the corresponding feature from the model. This is useful when you want to select a subset of features that have significant effects on the target variable.\n",
    "\n",
    "Ridge regularization, on the other hand, adds the squared magnitude of coefficient as penalty term. It usually results in smaller but non-zero coefficients. It's useful when all features have some importance and should be included in the model.\n",
    "\n",
    "Lasso is more suitable when you have many features and want to select only the most significant ones, while Ridge is more suitable when all features are important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quesiton 7\n",
    "\n",
    "1. Regularized linear models add a penalty term to the loss function, encouraging smaller coefficients.\n",
    "2. This helps prevent overfitting by creating a simpler, more generalized model.\n",
    "3. Ridge regularization encourages smaller, non-zero coefficients, while Lasso can set some coefficients to zero (feature selection).\n",
    "4. Regularized models can be visualized through learning curves, where training and validation errors converge.\n",
    "5. Regularization helps balance model complexity and generalization, improving performance on new data.\n",
    "\n",
    "example-:\n",
    "\n",
    "Suppose you have a dataset with 10 features and you want to build a linear regression model to predict a target variable. Without any regularization, the model might overfit the training data and perform poorly on new, unseen data. This is because the model could assign large, unrealistic coefficients to the features, leading to a complex model that fits the training data very well but fails to generalize.\n",
    "\n",
    "Now when we use regularization models than issue of overfitting is resolved and only the important feature are considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Question 8\n",
    "\n",
    "Limitations of Regularized Linear Models (Brief)\n",
    "\n",
    "1. Bias-Variance Tradeoff: Regularization increases bias to reduce variance, potentially leading to underfitting when the true relationship is complex or nonlinear.\n",
    "2. Feature Selection Challenges: Lasso may exclude important correlated features, while Ridge retains all variables, reducing interpretability in feature selection.\n",
    "3. Non-linearity in Relationships: Regularized models assume linearity, which can fail to capture nonlinear patterns unless combined with other techniques.\n",
    "4. Selection of Regularization Parameter: Choosing the right regularization strength (ùúÜ) can be difficult, with too much or too little leading to underfitting or overfitting.\n",
    "5. Interpretability: Shrinking coefficients can make the model harder to interpret, especially when many variables are involved.\n",
    "6. Data Requirements: Regularization is beneficial with many predictors or collinearity, but adds bias when only a few predictors are present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9\n",
    "\n",
    "According to the situation given in the question I would like to go with `Model B` as we know the the performance metrics of the models.\n",
    "\n",
    "- Model A - 10 RMSE\n",
    "- Model B - 8 MAE\n",
    "\n",
    "As RMSE is calculated by squaring the error so it makes it more sensitive to outliers where as MAE calculates the performace directly, hence this is the reason I will go with the `Model B`\n",
    "\n",
    "Limitation to choice of metrics:\n",
    "1. RMSE vs. MAE: RMSE is more sensitive to large errors (outliers), while MAE gives an equal penalty to all errors. If the dataset has outliers, RMSE might exaggerate the performance gap. If no outliers are present, MAE might understate their importance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10\n",
    "\n",
    "I would choose Model B (Lasso with Œª = 0.5).\n",
    "\n",
    "Reason:\n",
    "Lasso performs both regularization and feature selection by setting some coefficients to zero, which is beneficial if only a subset of features is important. With Œª = 0.5, it likely provides stronger regularization and feature reduction, improving interpretability.\n",
    "\n",
    "Limitation:\n",
    "Lasso can exclude important correlated features, potentially oversimplifying the model if variables are highly related."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
