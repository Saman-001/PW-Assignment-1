{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Lasso Regression, or Least Absolute Shrinkage and Selection Operator, is a regression technique that performs both variable selection and regularization. It differs from other regression techniques by adding a penalty term to the loss function equivalent to the absolute value of the coefficients. This penalty term encourages the model to shrink some coefficients to zero, effectively removing them from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "The main advantage of using Lasso Regression (Least Absolute Shrinkage and Selection Operator) in feature selection is its ability to perform automatic feature selection by shrinking the coefficients of less important features to exactly zero.\n",
    "\n",
    "This happens because Lasso applies an L1 regularization penalty to the regression model, which encourages sparsity in the model's coefficients. As a result, Lasso not only reduces overfitting but also eliminates irrelevant or less important features by setting their corresponding coefficients to zero. This leads to a simpler, more interpretable model that retains only the most significant features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "In Lasso Regression, coefficients are interpreted as follows:\n",
    "\n",
    "1. Non-zero coefficients: These represent features that are important to the model. Their magnitude indicates the strength and direction (positive or negative) of the relationship with the target variable.\n",
    "2. Zero coefficients: These indicate that the corresponding features have been excluded as irrelevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "The main tuning parameter in Lasso Regression is alpha (Î»), which controls the strength of the L1 regularization.\n",
    "\n",
    "1. Higher alpha: Increases regularization, shrinking more coefficients to zero. This leads to a simpler model with fewer features but may increase bias.\n",
    "2. Lower alpha: Reduces regularization, keeping more features, which can reduce bias but may increase overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "Yes, Lasso Regression can be used for non-linear regression problems by incorporating feature engineering techniques, such as polynomial features or basis functions, to transform the original features into a higher-dimensional space.\n",
    "\n",
    "This allows Lasso to capture non-linear relationships in the data while still applying L1 regularization for feature selection. However, Lasso itself remains a linear model, so the non-linearity comes from the transformed features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "The main difference between Ridge Regression and Lasso Regression is in the type of regularization:\n",
    "\n",
    "1. Ridge Regression uses L2 regularization, which shrinks coefficients but does not set them to zero. It penalizes large coefficients but keeps all features.\n",
    "2. Lasso Regression uses L1 regularization, which can shrink some coefficients to exactly zero, effectively performing feature selection by excluding irrelevant features.\n",
    "\n",
    "In short, Ridge keeps all features, while Lasso can eliminate unimportant ones.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "Yes, Lasso Regression can handle multicollinearity to some extent. It does so by shrinking the coefficients of correlated features, and in some cases, it may set the coefficients of certain correlated features to zero, effectively selecting one feature from a group of highly correlated ones while eliminating the others. This helps reduce multicollinearity's impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "\n",
    "To choose the optimal value of the regularization parameter (lambda) in Lasso Regression, you can use the following methods:\n",
    "\n",
    "1. Cross-Validation:\n",
    "\n",
    "    - Split the data into training and validation sets (commonly using k-fold cross-validation).\n",
    "    - Train the model on the training set with different lambda values.\n",
    "    - Evaluate the model's performance on the validation set for each lambda.\n",
    "    - Select the lambda that gives the best performance (e.g., lowest mean squared error).\n",
    "2. Grid Search:\n",
    "\n",
    "    - Define a range of lambda values to test.\n",
    "    - Use cross-validation to assess each lambda value's performance.\n",
    "    - Choose the lambda that optimizes the model's performance based on the validation set.\n",
    "3. Information Criteria:\n",
    "\n",
    "    - Use metrics like AIC (Akaike Information Criterion) or BIC (Bayesian Information Criterion) to evaluate the trade-off between model fit and complexity.\n",
    "    - Select the lambda that minimizes the chosen information criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
