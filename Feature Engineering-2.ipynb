{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2749fe4c-d901-479f-8e2e-b19840e8a8ad",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "The Filter method in feature selection is a technique used to select a subset of relevant features for model construction. It evaluates the relevance of each feature independently of the model, based on intrinsic properties of the data. \n",
    "\n",
    "Working of the Filter method:-\n",
    "1. Statistical Measures: Features are scored using statistical metrics such as:\n",
    "\n",
    "     1. Correlation: For continuous or ordinal variables.\n",
    "    2. Mutual Information: Measures dependency between variables.\n",
    "    3. Chi-Square Test: For categorical data.\n",
    "    4. Variance Threshold: Removes features with low variance.\n",
    "2. Rank Features: Features are ranked according to their scores.\n",
    "\n",
    "3. Select Features: A subset of features is chosen based on their ranking or a predefined threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0955e4-4ae4-4362-910f-577d3afcfac5",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "Differences Between Wrapper and Filter Methods in Feature Selection\n",
    "#### Wrapper Method\n",
    "1. Model-Based Evaluation:\n",
    "        Evaluates feature subsets based on model performance.\n",
    "        Involves training models with different subsets of features.\n",
    "2. Search Strategy:\n",
    "\n",
    "    Uses algorithms like forward selection, backward elimination, or recursive feature elimination.\n",
    "    Considers interactions between features.\n",
    "3. Performance Evaluation:\n",
    "\n",
    "    Utilizes cross-validation to assess each subset.\n",
    "4. Advantages:\n",
    "\n",
    "    Tailors feature subsets to specific models.\n",
    "    Accounts for feature interactions.\n",
    "5. Disadvantages:\n",
    "\n",
    "    Computationally intensive and time-consuming.\n",
    "    Risk of overfitting.\n",
    "    \n",
    "    \n",
    "#### Filter Method\n",
    "1. Statistical Measure-Based Evaluation:\n",
    "\n",
    "    Evaluates each feature independently using statistical metrics.\n",
    "    Examples include correlation, mutual information, Chi-square tests, and variance thresholds.\n",
    "2. Simple and Fast:\n",
    "\n",
    "    Quick and computationally efficient.\n",
    "    Does not involve model training.\n",
    "3. No Search Strategy:\n",
    "\n",
    "    Ranks features individually without considering combinations.\n",
    "4. Advantages:\n",
    "\n",
    "    Fast and model-agnostic.\n",
    "    Reduces overfitting risk.\n",
    "5. Disadvantages:\n",
    "\n",
    "    Ignores feature interactions.\n",
    "    May not yield the best subset for complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89c295b-3ef5-43de-b20d-6d218e83b941",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Common techniques in embedded feature selection methods include:\n",
    "\n",
    "1. Lasso Regression (L1 regularization)\n",
    "2. Ridge Regression (L2 regularization)\n",
    "3. Elastic Net (combination of L1 and L2 regularization)\n",
    "4. Decision Trees\n",
    "5. Random Forests\n",
    "6. Gradient Boosting Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b267e5c6-e489-4fcc-8caa-5554d869ca9a",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "Some drawbacks of using the Filter method for feature selection include:\n",
    "\n",
    "1. **Ignores Feature Interactions**: Considers features independently, missing interactions between them.\n",
    "2. **Model Agnostic**: Not tailored to specific models, which might affect performance.\n",
    "3. **Risk of Overlooking Important Features**: May overlook features that are individually weak but collectively strong.\n",
    "4. **Simplistic**: Uses basic statistical measures, which might not capture complex relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c5894e-1dc3-4e8a-ab91-82d3c8b89e0e",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "You would prefer using the Filter method over the Wrapper method in the following situations:\n",
    "\n",
    "1. **High-Dimensional Data**: When dealing with a large number of features.\n",
    "2. **Computational Efficiency**: When you need faster and less resource-intensive feature selection.\n",
    "3. **Preprocessing Step**: When you want a quick preliminary feature selection before applying more sophisticated methods.\n",
    "4. **Avoid Overfitting**: When you want to reduce the risk of overfitting in models with small datasets.\n",
    "5. **Model-Agnostic**: When you need a method that works independently of any specific machine learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e0d918-7ba8-4cb6-a85e-f9199bc29aa9",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "To choose the most pertinent features for customer churn prediction using the Filter Method:\n",
    "\n",
    "1. **Preprocess Data**: Handle missing values and encode categorical variables.\n",
    "2. **Compute Metrics**:\n",
    "   - **Correlation**: Measure correlation with the target variable.\n",
    "   - **Chi-Square**: For categorical features.\n",
    "   - **Mutual Information**: For dependency assessment.\n",
    "   - **ANOVA F-Value**: For numerical features.\n",
    "3. **Rank and Select Features**: Choose the top-ranked features based on the metrics.\n",
    "4. **Evaluate**: Optionally test the feature subset with a simple model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b7efc1-97b0-47b9-8ed2-798a7f8d2c14",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "To use the Embedded method for feature selection:\n",
    "\n",
    "1. **Choose a Model**: Pick a model with built-in feature selection (e.g., Lasso, Random Forest).\n",
    "2. **Train the Model**: Fit the model on your dataset.\n",
    "3. **Extract Feature Importance**: Get importance scores or coefficients from the trained model.\n",
    "4. **Select Features**: Choose features based on their importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1021406-9c76-4a0e-afbc-fdf5f536d32d",
   "metadata": {},
   "source": [
    "## Quesiton 8\n",
    "\n",
    "To use the Wrapper method for feature selection in predicting house prices:\n",
    "\n",
    "1. **Choose a Base Model**: Select a machine learning model to evaluate feature subsets (e.g., Linear Regression, Decision Tree).\n",
    "\n",
    "2. **Generate Feature Subsets**: Create different subsets of features (e.g., using methods like Forward Selection, Backward Elimination, or Recursive Feature Elimination).\n",
    "\n",
    "3. **Evaluate Subsets**: Train the model on each subset and evaluate its performance using metrics like Mean Squared Error (MSE) or R-squared.\n",
    "\n",
    "4. **Select the Best Subset**: Choose the feature subset that yields the best performance on the evaluation metric.\n",
    "\n",
    "5. **Refine if Necessary**: Optionally, iterate with different subsets or models to further optimize feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e98de9d-0c93-42e3-b903-54509581e460",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
