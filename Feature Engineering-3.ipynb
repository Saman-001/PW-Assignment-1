{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa452372-72d9-4033-a33a-7c005b53f0cf",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to transform the features of a dataset to lie within a specific range, typically [0, 1]. This is achieved by scaling the data such that the minimum value of each feature becomes 0 and the maximum value becomes 1. The formula for Min-Max scaling is:\n",
    "\n",
    "x' = x- Xmin / Xmax - Xmin\n",
    "\n",
    "\n",
    "e.g = suppose there are numerical features in a dataset and they ranges differently, so to overcome this problem we can use Min-Max Scaler. This will range all the numerical features in the range of 0-1.\n",
    "\n",
    " - total_bill feature in tips dataset.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db96647-a5ae-4472-b3df-600084e53ed5",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "The Unit Vector technique, also known as normalization or vector normalization, involves scaling the feature vector to have a unit norm. This means that the length (or magnitude) of the vector is scaled to 1. This technique is especially useful when the direction of the vector is more important than its magnitude, such as in text analysis or when working with unitless measures.\n",
    "\n",
    "X' = x / ||x||\n",
    "\n",
    "x is the original vector\n",
    "||x|| is the norm of the vector x\n",
    "\n",
    "Main difference between Min-Max and Unit vector scaling is:\n",
    "1. Min-Max Scaling: Scales features to a fixed range, typically [0, 1].\n",
    "2. Unit Vector Technique: Scales the entire vector to have a magnitude of 1, focusing on direction rather than individual feature range.\n",
    "\n",
    "\n",
    "    X1  X2\n",
    "\n",
    "\t3\t4\n",
    "\n",
    "\t1\t2\n",
    "\n",
    "\t0\t0\n",
    "\n",
    "\t5\t12\n",
    "\n",
    "\n",
    "After using unit vector\n",
    "\n",
    "    X1       X2\n",
    "\n",
    "\t0.6\t    0.8\n",
    "\n",
    "    0.45\t0.89\n",
    "\n",
    "\t0\t    0\n",
    "\n",
    "\t0.38\t0.92"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee485eab-868f-4328-a5ab-29cd9356db31",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "PCA stands for Principal Component Analysis. It is a technique used to reduce the features on which our ML model will be trained. \n",
    "\n",
    "It is used to select the most important features, in other words it converts multi-dimensional array into 2D or 1D array.\n",
    "\n",
    "But if we neglect all other features and focuses only on 1 feature this will reduce the efficiency of our ML model. To overcome this problem we will plot a orthogonal line AKA best line.\n",
    "\n",
    "e.g - \n",
    "\n",
    "    X1    X2\n",
    "    \n",
    "    2.5\t2.4\n",
    "\t0.5\t0.7\n",
    "\t2.2\t2.9\n",
    "\t1.9\t2.2\n",
    "\t3.1\t3.0\n",
    "\t2.3\t2.7\n",
    "\t2.0\t1.6\n",
    "\t1.0\t1.1\n",
    "\t1.5\t1.6\n",
    "\t1.1\t0.9\n",
    "    \n",
    "After implementing the Unit Vector technique:-\n",
    "\n",
    "\n",
    "    X'\n",
    "    3.07\n",
    "\t0.68\n",
    "\t3.28\n",
    "\t2.44\n",
    "\t4.24\n",
    "    2.89\n",
    "\t1.69\n",
    "\t0.69\n",
    "\t1.34\n",
    "\t0.55"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a9135a-c1bd-43c2-abfb-3d3b6988cd43",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Principal Component Analysis (PCA) is closely related to feature extraction because it identifies the directions (principal components) in which the variance of the data is maximized. These directions are linear combinations of the original features, and they capture the most important information in the data.\n",
    "\n",
    "Feature extraction using PCA involves transforming the original high-dimensional data into a lower-dimensional space. The new features (principal components) are uncorrelated, and they represent the most significant patterns in the data, which can simplify modeling and improve the performance of machine learning algorithms.\n",
    "\n",
    "How PCA Can Be Used for Feature Extraction\n",
    "1. Standardize the Data: Ensure each feature has zero mean and unit variance.\n",
    "2. Compute the Covariance Matrix: Understand the relationships between the features.\n",
    "3. Compute the Eigenvalues and Eigenvectors: Determine the principal components.\n",
    "4. Select Principal Components: Choose the top k principal components based on the largest eigenvalues.\n",
    "5. Transform the Data: Project the original data onto the selected principal components to obtain the new features.\n",
    "\n",
    "     X1      X2     X3\n",
    "     \n",
    "\t2.5\t     2.4\t1.8\n",
    "\t\n",
    "    0.5\t     0.7\t0.6\n",
    "\t\n",
    "    2.2\t     2.9    2.3\n",
    "\t\n",
    "    1.9\t     2.2\t1.9\n",
    "\t\n",
    "    3.1\t     3.0\t2.9\n",
    "    \n",
    "After using PCA for feature extraction.\n",
    "\n",
    "    PC1      PC2\n",
    "\t3.31\t-0.01\n",
    "\t0.82\t 0.04\n",
    "\t3.72\t-0.04\n",
    "\t2.77\t-0.03\n",
    "\t4.81\t-0.07"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb88998c-6788-40b3-bf26-27e27acd7c86",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "Features available in a dataset are as:\n",
    "1. price\n",
    "2. rating\n",
    "3. delivery_time\n",
    "\n",
    "These features will contain numerical data ranging very differentely.\n",
    "\n",
    "Let, \n",
    " - price - 1000,400,500,650,700 etc.\n",
    " - rating - 1,2,3,4,5.\n",
    " - delivery_time - 30,40,50,20 etc.\n",
    "        \n",
    "All these numerical values are very different and co-relation between them is very diffcult to find.\n",
    "\n",
    "So,Here we can use the Min-Max scaling to reduce the values and range all of them betweem 0-1. This can really help to reduce our computation and easily identify the relation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6873c69-d05a-4907-a66a-5a10dd1202ac",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "The given features are as:\n",
    "1. financial data.\n",
    "2. market trends.\n",
    "\n",
    "We can use PCA to convert the 2D array into 1D array i.e. reduce to 1 feature from 2 feature.\n",
    "\n",
    "We can plot a orhtogonal line in the graph and extract the most important feature from the given feature. Line consisting most of the data points is plotted and the points on the line are converted into 1D array. This 1D array can be used to train  a ML model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6b926e-6d0e-46a7-b9a1-c20eece9c21f",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "given values,\n",
    " - 1,5,10,15,20\n",
    "\n",
    "\n",
    "1 = 1-1/20-1 = 0\n",
    "\n",
    "5 = 5-1/20-1 = 0.21\n",
    "\n",
    "10 = 10-1/20-1 = 0.47\n",
    "\n",
    "15 = 15-1/20-1 = 0.73\n",
    "\n",
    "20 = 20-1/20-1 = 1\n",
    "\n",
    "Normalized data is: \n",
    " - 0,0.21,0.47,0.73,1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac163851-b88b-4695-af43-ea32e39785df",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "\n",
    "Given a dataset with features: [height, weight, age, gender, blood pressure], we will perform feature extraction using PCA.\n",
    "\n",
    "Steps for PCA\n",
    "1. Standardize the Data: Standardize features to have zero mean and unit variance.\n",
    "2. Compute Covariance Matrix: Calculate the covariance matrix of the standardized data.\n",
    "3. Compute Eigenvalues and Eigenvectors: Find eigenvalues and eigenvectors of the covariance matrix.\n",
    "4. Select Principal Components: Choose the top k principal components that explain the most variance.\n",
    "5. Transform the Data: Project data onto the selected principal components.\n",
    "\n",
    "For the dataset [height, weight, age, gender, blood pressure], retain three principal components to capture at least 95% of the variance, reducing dimensionality from five to three while preserving significant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15045b1d-d253-4f98-98d5-ef2192135d81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
