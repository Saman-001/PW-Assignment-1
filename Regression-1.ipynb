{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Simple Linear Regression:\n",
    "\n",
    "Involves one independent variable and one dependent variable.\n",
    "Models the relationship as a straight line.\n",
    "Example: Predicting a person's weight (dependent variable) based on their height (independent variable).\n",
    "\n",
    "Multiple Linear Regression:\n",
    "\n",
    "Involves two or more independent variables and one dependent variable.\n",
    "Models the relationship using a plane or hyperplane.\n",
    "Example: Predicting a house price (dependent variable) based on size, number of bedrooms, and location (independent variables)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    " Assumptions of Linear Regression\n",
    "1. Linearity: The relationship between the independent and dependent variables is linear.\n",
    "- Check: Scatter plots of predicted vs. actual values; residual plots.\n",
    "2.  Independence: Observations are independent of each other.\n",
    "- Check: Plot time series data or use the Durbin-Watson test.\n",
    "3.  Homoscedasticity: Constant variance of residuals across all levels of the independent variables.\n",
    "\n",
    "- Check: Residual plots should show no patterns; use Breusch-Pagan test.\n",
    "4.  Normality of Residuals: Residuals should be approximately normally distributed.\n",
    "\n",
    "-  Check: Q-Q plots or histograms of residuals; Shapiro-Wilk test.\n",
    "5. No multicollinearity: Independent variables should not be highly correlated.\n",
    "\n",
    "- Check: Variance Inflation Factor (VIF) analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Interpretation of Slope and Intercept in Linear Regression\n",
    "\n",
    "Intercept (𝑏0​): The expected value of the dependent variable when all independent variables are zero. It represents the baseline level.\n",
    "\n",
    "Slope (𝑏1): The change in the dependent variable for a one-unit increase in the independent variable, holding all other variables constant. It shows the strength and direction of the relationship.\n",
    "\n",
    "Example: Predicting Salary Based on Years of Experience\n",
    "\n",
    "Model: \n",
    "\n",
    "Salary=𝑏0+𝑏1×Years of Experience\n",
    "\n",
    "Salary=b0​ + b1 × Years of Experience\n",
    "\n",
    "Assume the regression equation is: \n",
    "\n",
    "Salary=30,000+5,000×Years of Experience\n",
    "\n",
    "\n",
    "Intercept (30,000): If a person has 0 years of experience, their expected salary is $30,000.\n",
    "\n",
    "Slope (5,000): For each additional year of experience, the salary is expected to increase by $5,000.\n",
    "\n",
    "This means if someone has 3 years of experience, their expected salary would be: \n",
    "\n",
    "Salary=30,000+5,000×3=45,000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Concept: Gradient descent is an optimization algorithm used to minimize a function by iteratively moving towards the steepest descent, defined by the negative gradient. It helps find the optimal parameters (weights) in models, such as linear regression.\n",
    "\n",
    "Uses :-\n",
    "\n",
    "1. Model Training: Gradient descent is commonly used to optimize the loss function in various algorithms, including linear regression, logistic regression, and neural networks.\n",
    "\n",
    "2. Hyperparameter Tuning: Adjusting the learning rate and the number of iterations can significantly affect model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "Multiple linear regression (MLR) is a statistical technique that models the relationship between one dependent variable and two or more independent variables. The general form of the MLR equation is:\n",
    "y = β0 + β1x1 + β2x2 + … + βnxn\n",
    "\n",
    "Differences from Simple Linear Regression\n",
    "1. Number of Variables: Simple linear regression has one independent variable, while multiple linear regression has two or more independent variables.\n",
    "\n",
    "2. Model Complexity: Simple linear regression represents a straight line, whereas multiple linear regression represents a hyperplane in multi-dimensional space.\n",
    "\n",
    "3. Interpretation: In simple regression, the slope indicates the change in the dependent variable for a one-unit change in the independent variable; in multiple regression, each coefficient shows the change for a one-unit change in that variable, holding others constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "Multicollinearity in Multiple Linear Regression\n",
    "\n",
    "Concept: Multicollinearity occurs when independent variables are highly correlated, leading to unstable coefficient estimates and interpretation difficulties.\n",
    "\n",
    "#### Detection\n",
    "1. Correlation Matrix: Check for high correlation coefficients among independent variables.\n",
    "2. Variance Inflation Factor (VIF): VIF > 5 or 10 indicates multicollinearity.\n",
    "3. Condition Index: A condition index above 30 suggests multicollinearity.\n",
    "\n",
    "#### Addressing Multicollinearity\n",
    "1. Remove Variables: Exclude one of the correlated variables.\n",
    "\n",
    "2. Combine Variables: Create a composite variable.\n",
    "\n",
    "3. Use Regularization: Apply Ridge or Lasso regression.\n",
    "\n",
    "4. Principal Component Analysis (PCA): Transform correlated variables into uncorrelated principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "Definition: Polynomial regression is a type of regression analysis that models the relationship between the dependent variable and one or more independent variables as an \n",
    "nth degree polynomial. The general form is:\n",
    "\n",
    "Y =b0 + b1X2 + b2 X3 + b3X3 +......+ bn Xn\n",
    "\n",
    "#### Differences from Linear Regression\n",
    "1. Function Form: Linear regression models a straight line, while polynomial regression models a curved line using polynomial terms.\n",
    "2. Complexity: Linear regression is limited to first-degree equations, whereas polynomial regression can use higher-degree equations for flexibility.\n",
    "3. Interpretation: In linear regression, each coefficient represents a constant change, while in polynomial regression, coefficients represent varying changes based on the value of X and its powers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "\n",
    "#### Advantages of Polynomial Regression\n",
    "1. Captures Non-linearity: Models complex relationships better than linear regression.\n",
    "2. Flexibility: Can fit curves and trends in data that linear regression cannot.\n",
    "\n",
    "#### Disadvantages of Polynomial Regression\n",
    "1. Overfitting: Higher degrees can lead to overfitting, making the model sensitive to noise.\n",
    "2. Complexity: More complex models can be harder to interpret.\n",
    "3. Increased Computation: More parameters to estimate can require more computation.\n",
    "\n",
    "#### Situations to Prefer Polynomial Regression\n",
    "1. When the relationship between variables is non-linear.\n",
    "2. When data exhibits a curvilinear pattern that linear regression cannot adequately model.\n",
    "3. When improving fit is critical, and the risk of overfitting can be managed.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
