{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "1. Output:\n",
    "   - Linear Regression: Predicts a continuous value (e.g., house price).\n",
    "   - Logistic Regression: Predicts a categorical outcome, typically binary (e.g., yes/no, 1/0).\n",
    "\n",
    "2. Mathematical Approach:\n",
    "- Linear: Uses a linear equation Y=β0+β1X.\n",
    "- Logistic: Uses the sigmoid function to produce a probability between 0 and 1.\n",
    "3. Loss Function:\n",
    "- Linear: Minimizes Mean Squared Error (MSE).\n",
    "- Logistic: Minimizes Log-Loss (Cross-Entropy).\n",
    "\n",
    "4. Assumptions:\n",
    "\n",
    "- Linear: Assumes a linear relationship and normally distributed errors.\n",
    "- Logistic: Assumes log-odds are linearly related to features.\n",
    "\n",
    "Example for Logistic Regression\n",
    "- Predicting if a patient has a disease (yes/no) based on diagnostic data is better suited for logistic regression due to the binary nature of the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "The cost function used in logistic regression is log-loss (cross-entropy loss), defined as:\n",
    "\n",
    "J(θ)= −1m∑i=1m[y(i)log⁡(hθ(x(i)))+(1−y(i))log⁡(1−hθ(x(i)))]\n",
    "​\n",
    "- (x) is the predicted probability, \n",
    "- y is the actual label, and \n",
    "- m is the number of samples.\n",
    "\n",
    "Optimization:\n",
    "It is typically optimized using gradient descent (or variants like stochastic gradient descent), updating the weights iteratively to minimize the cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Regularization in logistic regression adds a penalty term to the cost function to prevent overfitting, which occurs when the model fits the training data too closely and performs poorly on unseen data.\n",
    "\n",
    "Types of Regularization:\n",
    "1. L2 Regularization (Ridge): Adds a penalty proportional to the square of the coefficients (weights) to the cost function:\n",
    "    - J(θ)=−1m∑...+λ∑j=1nθj2\n",
    "\n",
    "2. L1 Regularization (Lasso): Adds a penalty proportional to the absolute value of the coefficients:\n",
    "     - J(θ)=−1m∑...+λ∑j=1n∣θj∣\n",
    "\n",
    "\n",
    "How It Helps Prevent Overfitting:\n",
    "Regularization discourages large coefficient values, thus simplifying the model and reducing variance. This helps the model generalize better to new data by avoiding overfitting to the noise or complex patterns in the training set. The parameter λ controls the strength of the regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "The ROC (Receiver Operating Characteristic) curve is a graphical tool used to evaluate the performance of a binary classification model, like logistic regression.\n",
    "\n",
    "Components of the ROC Curve:\n",
    "1. True Positive Rate (TPR): Also known as sensitivity or recall, it measures the proportion of actual positives correctly predicted: \n",
    "    - TPR=TP/TP+FN.\n",
    "\n",
    "2. False Positive Rate (FPR): It measures the proportion of actual negatives incorrectly predicted as positives: \n",
    "    - FPR=FP/FP+TN.\n",
    "\n",
    "The ROC curve plots TPR against FPR at various threshold settings of the model.\n",
    "\n",
    "How It’s Used:\n",
    "Evaluation: The closer the ROC curve is to the top-left corner, the better the model performs.\n",
    "\n",
    "AUC (Area Under the Curve): A single-number summary of the ROC curve. An AUC of 1 indicates a perfect model, while 0.5 indicates random guessing.\n",
    "The ROC curve helps assess the model's ability to distinguish between classes across different thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "Common Techniques for Feature Selection in Logistic Regression:\n",
    "\n",
    "1. Recursive Feature Elimination (RFE):\n",
    "Iteratively removes the least important features based on model performance.\n",
    "Helps by simplifying the model, improving interpretability, and reducing overfitting.\n",
    "\n",
    "2. L1 Regularization (Lasso):\n",
    "Penalizes the absolute values of coefficients, driving some to zero.\n",
    "Helps automatically select the most important features by eliminating irrelevant ones.\n",
    "3. Univariate Statistical Tests (e.g., Chi-Square, ANOVA):\n",
    "Selects features based on their statistical significance with the target variable.\n",
    "Improves performance by choosing features that have the strongest relationship with the outcome.\n",
    "4. Principal Component Analysis (PCA):\n",
    "Reduces dimensionality by transforming features into principal components that capture the most variance.\n",
    "Helps by reducing noise and improving computational efficiency.\n",
    "5. Correlation Matrix with Threshold:\n",
    "Removes highly correlated features (multicollinearity) that provide redundant information.\n",
    "Improves stability and prevents inflated coefficients in the model.\n",
    "\n",
    "How These Techniques Improve Performance:\n",
    "1. Reduce Overfitting: By eliminating irrelevant or redundant features, the model generalizes better to new data.\n",
    "2. Improve Interpretability: Fewer features make the model easier to understand and interpret.\n",
    "3. Enhance Efficiency: Reducing the number of features can lower computational cost and training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "Handling imbalanced datasets in logistic regression is crucial for preventing bias toward the majority class. Here are some common strategies to address class imbalance:\n",
    "\n",
    "1. Resampling Techniques:\n",
    "    - Oversampling the Minority Class: Increase the number of instances in the minority class (e.g., using SMOTE - Synthetic Minority Over-sampling Technique).\n",
    "    - Undersampling the Majority Class: Reduce the number of instances in the majority class to balance the dataset.\n",
    "    - Combination of Both: A balanced mix of oversampling the minority and undersampling the majority class.\n",
    "\n",
    "2. Class Weighting:\n",
    "Assign higher class weights to the minority class in the logistic regression model. This forces the model to pay more attention to the minority class by penalizing misclassification of minority class examples more heavily.\n",
    "\n",
    "3. Anomaly Detection:\n",
    "When the minority class is rare (e.g., fraud detection), treat it as an anomaly detection problem instead of a standard classification problem.\n",
    "\n",
    "4. Threshold Tuning:\n",
    "Instead of using the default 0.5 probability threshold, adjust the decision threshold to better balance the precision and recall for the minority class.\n",
    "\n",
    "5. Use Evaluation Metrics for Imbalanced Data:\n",
    "Focus on metrics like Precision, Recall, F1-score, and AUC-ROC instead of accuracy, which can be misleading for imbalanced datasets.\n",
    "\n",
    "How These Strategies Help:\n",
    "They ensure that the model pays appropriate attention to the minority class, improving its ability to correctly classify minority class instances without being overwhelmed by the majority class, thus improving overall performance and fairness in predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "1. Multicollinearity:\n",
    "    - Problem: Correlated independent variables cause unstable coefficients.\n",
    "    - Solution: Use VIF to remove correlated features, apply L2 regularization, or use PCA.\n",
    "\n",
    "2. Overfitting:\n",
    "    - Problem: The model fits noise in the data.\n",
    "    - Solution: Use regularization, apply feature selection, and validate with cross-validation.\n",
    "\n",
    "3. Imbalanced Data:\n",
    "    - Problem: The model may favor the majority class.\n",
    "    - Solution: Use oversampling/undersampling, apply class weights, and focus on precision, recall, or AUC-ROC.\n",
    "\n",
    "4. Non-linearity:\n",
    "    - Problem: Logistic regression assumes linear relationships.\n",
    "    - Solution: Add polynomial features, interaction terms, or consider non-linear models.\n",
    "\n",
    "5. Outliers:\n",
    "    - Problem: Outliers can skew predictions.\n",
    "    - Solution: Remove outliers or apply robust scaling techniques.\n",
    "\n",
    "6. Convergence Issues:\n",
    "    - Problem: The algorithm fails to converge.\n",
    "    - Solution: Scale features, reduce feature count, or use regularization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
